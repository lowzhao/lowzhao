---
published: true
layout: post
author: eugene
categories: Machine_Learning
tags:
- featured
- faiss
- NLP
- bert
- sts
- sentence-similarity
image: https://i0.wp.com/blog.exxactcorp.com/wp-content/uploads/2019/05/1_blSbN23mOGMZ_DWvTAcO1w.png
---

## State of the Art Semantic Textual Similarity

With Google newly released [BERT]. There is lots of extension of models available online(ie. [huggingface/transformers], and [UKPLab/sentence-transformers]). This article focuses of plug and play code snippets and I will provide a sample code that you can modify to your needs. But keep in mind that these models are supposed to be fine-tuned for your specific use cases.

After that we will setup a [facebookresearch/faiss] index for scalability of deployment. 

I attempted to use default [BERT] model from [huggingface/transformers] but there is a problem, there is no good embedding models return from the models(read return type of https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel for more). Fine-tuning process is absolutely required, but there is usually either _no great dataset_ , _require too deep understanding_ and _large computing resources_, which I don't not posses. Besides, because I am in Hong Kong, most of the task required Chinese and English multilingual models, I choose to use this `bert-base-multilingual-cased` from https://huggingface.co/transformers/pretrained_models.html.

### Installation

```bash
pip install transformers, pytorch, hanziconv 
```

### Traditional to Modern Chinese
In Hong Kong most of the application are in traditional chinese, for this I used the tokenizer to test if the encoding is the same
```python
tokenizer.encode('爱')
tokenizer.encode('愛')
```

```
> [101, 5383, 102]
> [101, 3910, 102]
```
The response is they are different.

In that case my dataset is a mixed of tranditional and modern Chinese, I decided to use a package [berniey/hanziconv] for this conversion.

```python
from hanziconv import HanziConv as t2mc

t2mc.toSimplified('愛')
```

```
> '爱'
```

### Using `bert-base-multilingual-cased`
`bert-base-multilingual-cased` from [huggingface/transformers].
```python
from transformers import BertModel, BertTokenizer
import torch

bert_model_shortcut = 'bert-base-multilingual-cased'

tokenizer = BertTokenizer.from_pretrained(bert_model_shortcut)
model = BertModel.from_pretrained(bert_model_shortcut)

encoded_tokens = tokenizer.encode("Here is some text to encode", add_special_tokens=True)

input_ids = torch.tensor([encoded_tokens])

with torch.no_grad():
    last_hidden_states = model(input_ids)

last_hidden_states.shape 

```

```python
```
[BERT]: https://github.com/google-research/bert
[huggingface/transformers]: https://huggingface.co/
[facebookresearch/faiss]: https://github.com/facebookresearch/faiss
[UKPLab/sentence-transformers]: https://github.com/UKPLab/sentence-transformers
[berniey/hanziconv]: https://github.com/berniey/hanziconv
[Transformer Image](https://arxiv.org/abs/1706.03762)
